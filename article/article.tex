% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\documentclass[
]{article}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={TRANSFORMANDO APIS EM INTERFACES CONVERSACIONAIS: VALIDAÇÃO DA ABORDAGEM OPENAPI-MCP PARA AGENTES BASEADOS EM IA},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{\textbf{TRANSFORMANDO APIS EM INTERFACES CONVERSACIONAIS:
VALIDAÇÃO DA ABORDAGEM OPENAPI-MCP PARA AGENTES BASEADOS EM IA}}
\author{}
\date{}

\begin{document}
\maketitle

\subsubsection{Artigo em produção - Checklist de
produção}\label{artigo-em-produuxe7uxe3o---checklist-de-produuxe7uxe3o}

\begin{itemize}
\tightlist
\item[$\square$]
  Edição do artigo

  \begin{itemize}
  \tightlist
  \item[$\square$]
    Aplicar formatação da SATC

    \begin{itemize}
    \tightlist
    \item[$\square$]
      Definir o template do .docx com o Word
    \end{itemize}
  \item[$\boxtimes$]
    Referências

    \begin{itemize}
    \tightlist
    \item[$\boxtimes$]
      Formatação ABNT
    \end{itemize}
  \end{itemize}
\item[$\square$]
  Escrita

  \begin{itemize}
  \tightlist
  \item[$\boxtimes$]
    Resumo
  \item[$\boxtimes$]
    Introdução
  \item[$\boxtimes$]
    Material e métodos
  \item[$\boxtimes$]
    Revisão e entrega parcial (nota 4.5/5)
  \item[$\square$]
    Desenvolvimento
  \item[$\square$]
    Resultados e discussão
  \item[$\square$]
    Considerações finais
  \item[$\square$]
    Revisão após finalizar o artigo
  \end{itemize}
\end{itemize}

\textbf{Lucas de Castro Zanoni}\footnote{Graduando em Engenharia de
  software no semestre letivo de 2025-1. E-mail:
  castro.lucas290@gmail.com}

\textbf{Thyerri Fernandes Mezzari}\footnote{Professor do Centro
  Universitário UniSATC E-mail: thyerri.mezzari@satc.edu.br}

Resumo: Este trabalho apresenta um estudo experimental de integração de
agentes conversacionais baseados em inteligência artificial a soluções
web através da especificação OpenAPI combinada com o protocolo Model
Context Protocol (MCP). A pesquisa investiga como especificações OpenAPI
podem ser automaticamente convertidas em servidores MCP, permitindo que
modelos de linguagem de grande escala (LLMs) interajam de forma
padronizada e segura com sistemas externos. Para garantir uma análise
rigorosa e reprodutível, foi desenvolvida uma interface padronizada e
definidos critérios objetivos, fundamentando-se em referências
acadêmicas, guias de segurança, relatórios de mercado e documentações
oficiais de provedores de modelos de linguagem. O estudo envolveu a
implementação de uma prova de conceito que inclui um gerador automático
de servidores MCP a partir de especificações OpenAPI, um cliente de chat
capaz de gerenciar múltiplos servidores MCP simultaneamente, e
aplicações de teste para validação da abordagem. Foram aplicados testes
automatizados end-to-end, com ênfase em métricas de robustez, segurança
(incluindo red teaming e injeção de prompts) e usabilidade. Os
resultados demonstram a viabilidade e eficácia da integração
OpenAPI-MCP, fornecendo uma análise fundamentada sobre os benefícios,
desafios e limitações desta abordagem para a integração de agentes
conversacionais em sistemas complexos, promovendo acessibilidade,
usabilidade e confiabilidade.

\textbf{Palavras-chave:} agente conversacional, integração de sistemas,
inteligência artificial, OpenAPI, Model Context Protocol, segurança,
usabilidade.

\section{1 INTRODUÇÃO}\label{introduuxe7uxe3o}

A evolução das interfaces de usuário tem gerado uma diversidade de
padrões de design e usabilidade, resultando frequentemente em barreiras
para a plena acessibilidade e interação dos usuários com os sistemas
digitais. Com o aumento da complexidade do frontend e a multiplicidade
de paradigmas de interação, muitos usuários enfrentam dificuldades
significativas para utilizar efetivamente as funcionalidades oferecidas
pelas soluções web modernas (RAPP et al., 2018) (KOCABALLI et al.,
2019). Nesse contexto, a ascensão dos Modelos de Linguagem de Grande
Escala (LLMs), como os desenvolvidos por OpenAI, Anthropic e Google, tem
impulsionado o desenvolvimento de agentes conversacionais mais avançados
e adaptáveis (ANTHROPIC, 2024a; OPENAI, 2022). Nos últimos anos, avanços
em modelos baseados em Transformer, como o BERT (2018), que aprimorou a
compreensão textual, e o GPT-3 (2020), que ampliou as capacidades
generativas e o aprendizado com poucos exemplos (\emph{few-shot}),
permitiram que os LLMs realizassem tarefas cada vez mais complexas a
partir de simples instruções em linguagem natural. Esses avanços
consolidaram os LLMs como interfaces conversacionais robustas e eficazes
para integração com sistemas.

Diante desse cenário, estudos recentes têm demonstrado que agentes
conversacionais podem aprimorar significativamente a experiência do
usuário ao simplificar interações com sistemas complexos (FAST et al.,
2017). Além disso, a implementação de interfaces baseadas em linguagem
natural tem mostrado potencial para melhorar a usabilidade em contextos
domésticos e inteligentes, reduzindo o tempo e o esforço necessários
para completar tarefas complexas (GUO et al., 2024). Ademais, tais
interfaces oferecem vantagens consideráveis em termos de acessibilidade,
permitindo uma comunicação mais inclusiva e adaptável a usuários com
diferentes necessidades especiais (LISTER et al., 2020) (DENG, 2023).
Para que esses benefícios sejam efetivamente alcançados em soluções web,
é fundamental avaliar as diferentes estratégias de integração desses
agentes aos sistemas existentes.

Nesse sentido, este estudo aborda experimentalmente a integração de
agentes conversacionais baseados em IA a sistemas web através da
especificação OpenAPI combinada com o protocolo emergente MCP (Model
Context Protocol). Esta abordagem permite que especificações OpenAPI
sejam automaticamente convertidas em servidores MCP, criando uma ponte
padronizada entre modelos de linguagem e sistemas externos. A solução
será avaliada quanto a desempenho, segurança, facilidade de
implementação e experiência do usuário, com foco específico na
capacidade de gerenciar múltiplos servidores MCP simultaneamente e na
eficácia da geração automática de código.

Dessa forma, a problemática central desta pesquisa reside na questão:
como a combinação da especificação OpenAPI com o protocolo MCP pode
facilitar a integração eficiente e segura de agentes conversacionais
baseados em IA com sistemas web existentes? Essa pergunta reflete a
necessidade crescente de soluções padronizadas que democratizem o acesso
à tecnologia, reduzindo a complexidade de integração e tornando sistemas
especializados mais acessíveis através de interfaces conversacionais
naturais.

A relevância deste estudo evidencia-se pelo potencial transformador que
os agentes conversacionais representam para a área de interação
humano-computador. Ao implementar um sistema intermediário capaz de
interpretar linguagem natural e traduzi-la em ações específicas dentro
de um sistema, cria-se uma ponte que permite aos usuários interagir de
forma mais intuitiva e natural com as tecnologias digitais. Esta
abordagem tem o potencial de mitigar as barreiras impostas por
interfaces complexas, contribuindo para uma maior inclusão digital e
para a melhoria da experiência do usuário em diversos contextos de
aplicação.

\section{2 PROCEDIMENTO EXPERIMENTAL}\label{procedimento-experimental}

Este estudo adota uma abordagem experimental estruturada em etapas
sequenciais para investigar a viabilidade e eficácia da integração de
agentes conversacionais baseados em IA a sistemas web através da
especificação OpenAPI combinada com o protocolo Model Context Protocol
(MCP). A pesquisa será examinada com base em uma prova de conceito
prática, desenvolvida para validar sua viabilidade técnica e avaliar
objetivamente aspectos funcionais e não-funcionais da solução proposta.

Inicialmente, será conduzida uma revisão sistemática da literatura,
consolidando conhecimentos científicos sobre integração OpenAPI-MCP e
embasando teoricamente a fase experimental. Na sequência, a estratégia
será implementada e testada por meio de uma prova de conceito
abrangente, incluindo o desenvolvimento de um gerador automático de
servidores MCP, um cliente de chat para gerenciamento de múltiplos
servidores, e aplicações de teste para validação da abordagem.

Os critérios de avaliação definidos incluem desempenho, segurança,
facilidade de implementação, manutenibilidade e experiência do usuário.
Para assegurar resultados objetivos e reproduzíveis, os testes incluirão
análises automatizadas end-to-end, medidas de robustez e segurança (como
testes de red teaming e proteção contra injeção de prompts) e avaliações
qualitativas de usabilidade. Os resultados serão sistematicamente
documentados e analisados, permitindo identificar desafios, vantagens e
limitações intrínsecas à integração OpenAPI-MCP e demonstrando sua
aplicabilidade prática para diferentes contextos de uso.

\subsection{2.1 MATERIAIS}\label{materiais}

Para garantir a rigorosidade científica e a reprodutibilidade dos
experimentos conduzidos neste estudo, é essencial uma seleção criteriosa
dos materiais e ferramentas utilizados. Esta seção detalha os recursos
específicos empregados na condução desta pesquisa, justificando sua
escolha baseada na eficiência, popularidade, robustez e aplicabilidade
prática dentro do contexto dos agentes conversacionais e integração de
sistemas.

\subsubsection{2.1.1 NODE.JS PARA DESENVOLVIMENTO DAS PROVAS DE
CONCEITO}\label{node.js-para-desenvolvimento-das-provas-de-conceito}

Node.js foi escolhido como plataforma principal para o desenvolvimento
das provas de conceito devido à sua comprovada eficácia na integração de
sistemas baseados em inteligência artificial (IA), especialmente com
agentes conversacionais e LLMs. A plataforma é amplamente adotada devido
à sua arquitetura orientada a eventos e capacidade de gerenciar
eficientemente múltiplas conexões simultâneas, essencial para aplicações
que exigem respostas rápidas em tempo real (CHEREDNICHENKO et al.,
2024).

Relatórios da \emph{Red Hat} destacam que o uso eficiente da arquitetura
assíncrona do Node.js possibilita a criação de agentes baseados em LLMs
com alta performance e escalabilidade. Isso garante um gerenciamento
eficiente de múltiplas operações paralelas, essencial para aplicações
intensivas em IA e integração com APIs externas (BLOG, 2024).

\subsubsection{\texorpdfstring{2.1.2 TESTES \emph{END-TO-END}
(E2E)}{2.1.2 TESTES END-TO-END (E2E)}}\label{testes-end-to-end-e2e}

O \emph{Framework} de Gerenciamento de Riscos de IA do NIST (OPREA;
VASSILEV, 2023) destaca a importância de avaliar o desempenho de
sistemas de IA de forma abrangente, defendendo que testes de integração
devem avaliar os sistemas de ponta a ponta para identificar erros de
integração e garantir a precisão das respostas em cenários realistas.
Testes rigorosos como esses não apenas identificam problemas de
integração, mas também asseguram às partes interessadas que o sistema se
comporta conforme o esperado em condições do mundo real.

A injeção de \emph{prompt} representa um risco significativo em
implantações de LLMs em nosso cenário, no qual o modelo possui acesso a
dados e sistemas potencialmente críticos, incluindo, ocasionalmente,
conexões diretas com dados brutos de banco de dados. O guia de riscos da
OWASP (JOHN et al., 2025) classifica a injeção de \emph{prompt} como uma
ameaça crítica à segurança, destacando a necessidade de procedimentos de
teste rigorosos para garantir que agentes conversacionais baseados em
LLMs não revelem inadvertidamente dados sensíveis ou contornem
restrições do sistema quando expostos a entradas maliciosas.
Recentemente, Wu et al.~(2023) (WU et al., 2023) demonstraram que
ataques de \emph{jailbreak} --- um tipo avançado de injeção de
\emph{prompt} --- podem burlar as salvaguardas éticas de modelos como o
ChatGPT em até 67\% dos casos, gerando conteúdos prejudiciais como
extorsão e desinformação.

Com isso em mente, o uso de testes E2E pode ser utilizado para avaliar a
resiliência da implementação ao simular entradas adversárias, processo
conhecido como \emph{red teaming}. Segundo Inie et al.~(2025) (INIE;
STRAY; DERCZYNSKI, 2025), o \emph{red teaming} desafia sistematicamente
sistemas de IA com \emph{prompts} adversários projetados para testar
seus limites e mecanismos de segurança. Ao encapsular consultas do
usuário com lembretes de responsabilidade ética (e.g., ``Você deve ser
um ChatGPT responsável''), o método reduziu a taxa de sucesso de
\emph{jailbreaks} para 19\%, mantendo a funcionalidade padrão do modelo
--- um resultado validado através de testes E2E em 540 cenários
adversarialmente projetados (WU et al., 2023).

\subsubsection{2.1.3 MODELOS DE LINGUAGEM DE GRANDE ESCALA
(LLMs)}\label{modelos-de-linguagem-de-grande-escala-llms}

Os LLMs, incluindo tecnologias como OpenAI GPT, Anthropic e modelos
disponibilizados pela Google, são essenciais neste estudo devido à sua
capacidade de interpretar e gerar linguagem natural de forma avançada e
eficaz. Estes modelos foram selecionados por sua performance comprovada
e ampla adoção em pesquisas acadêmicas e no mercado corporativo,
proporcionando um sólido embasamento para as funcionalidades de
interação do agente conversacional.

\paragraph{2.1.3.1 HISTÓRICO DO DESENVOLVIMENTO DE LLMS
(2018--2023)}\label{histuxf3rico-do-desenvolvimento-de-llms-20182023}

Nos últimos cinco anos, os LLMs evoluíram rapidamente, a partir da
arquitetura Transformer. O lançamento do BERT (2018) mostrou avanços em
compreensão textual, enquanto a série GPT demonstrou fortes capacidades
generativas. O GPT-3 (2020), com 175 bilhões de parâmetros, evidenciou
habilidades emergentes de aprendizado com poucos exemplos
(\emph{few-shot}), ampliando o escopo de tarefas possíveis por meio de
simples instruções em linguagem natural (BROWN et al., 2020).

A partir de 2022, o foco da pesquisa passou a ser o aprimoramento do
raciocínio e alinhamento dos LLMs. Técnicas como \emph{Chain-of-Thought
prompting} permitiram que os modelos resolvessem problemas complexos de
forma mais eficaz (WEI et al., 2023). O uso de Reinforcement Learning
from Human Feedback (RLHF), como nos modelos InstructGPT e
posteriormente ChatGPT, melhorou a capacidade dos LLMs de seguir
instruções com mais segurança e consistência. Esses avanços
estabeleceram as bases para o uso dos LLMs como interfaces
conversacionais robustas em cenários de integração com sistemas (OPENAI,
2022).

\paragraph{2.1.3.2 EXTENSÃO DE JANELA DE
CONTEXTO}\label{extensuxe3o-de-janela-de-contexto}

Com o avanço dos modelos, observou-se uma tendência significativa no
aumento das janelas de contexto --- a quantidade de tokens que um LLM
pode processar em uma única interação. Modelos como o Claude 3 já
alcançam até 100.000 tokens (ANTHROPIC, 2024b), enquanto versões
estendidas do GPT-4 suportam até 32.000 tokens (OPENAI, 2023a). Esse
aumento permite que os modelos processem documentos extensos, múltiplas
conversas ou grandes volumes de dados em uma única solicitação,
superando, em muitos casos, abordagens tradicionais baseadas em
retrieval-augmented generation (RAG), especialmente em tarefas que
exigem síntese contextual profunda.

A capacidade de manter longos contextos é altamente benéfica para
integração com sistemas -- um LLM pode manter diálogos prolongados,
lembrar estados extensos ou ingerir bancos de dados e logs inteiros de
uma só vez. No entanto, isso traz custos computacionais consideráveis, e
há esforços contínuos para utilizar essas janelas maiores de forma
eficiente (por exemplo, condensando ou focando a atenção nas partes mais
relevantes) (ANTHROPIC, 2024b; OPENAI, 2023a).

\paragraph{2.1.3.3 RACIOCÍNIO APRIMORADO E COMPREENSÃO PROFUNDA (DEEP
THINKING)}\label{raciocuxednio-aprimorado-e-compreensuxe3o-profunda-deep-thinking}

Os LLMs mais recentes apresentam avanços significativos em raciocínio,
planejamento e resolução de tarefas complexas. Técnicas como o
\emph{Chain-of-Thought prompting}, que induz os modelos a pensar em
etapas intermediárias, mostraram ganhos substanciais em tarefas que
exigem múltiplos passos lógicos (WEI et al., 2023). Além disso,
abordagens como \emph{tree-of-thought} e \emph{self-reflection} permitem
que os modelos reavaliem suas respostas e melhorem sua própria
performance iterativamente. Esses avanços tornam os LLMs mais confiáveis
para tarefas que exigem raciocínio profundo e tomada de decisão
estruturada, fundamentais para integração com sistemas complexos (YAO et
al., 2023).

\paragraph{2.1.3.4 USO DE FERRAMENTAS EM TEMPO REAL E INTERAÇÃO COM
SISTEMAS}\label{uso-de-ferramentas-em-tempo-real-e-interauxe7uxe3o-com-sistemas}

O avanço dos LLMs em ambientes de produção foi impulsionado por recursos
como o \emph{function calling} da OpenAI. Essa funcionalidade permite
que os modelos interpretem solicitações em linguagem natural e as
convertam em chamadas de funções estruturadas, conforme definido pelo
desenvolvedor. Por exemplo, ao receber uma instrução como ``agende uma
reunião para amanhã às 14h'', o modelo pode gerar uma chamada de função
com os parâmetros apropriados para interagir com uma API de calendário,
sem depender de engenharia de \emph{prompt} ou extração de texto
(OPENAI, 2023b). Essa abordagem, melhora significativamente a
confiabilidade em cenários de integração, permitindo que o modelo
obtenha dados estruturados de bancos de dados, chame APIs de negócios,
envie e-mails, entre outras ações, em vez de apenas tentar adivinhar a
resposta (OPENAI, 2023b).

Complementando essa capacidade, o \emph{Model Context Protocol} (MCP),
desenvolvido pela Anthropic (ANTHROPIC, 2024a; MODEL CONTEXT PROTOCOL
TEAM, 2025), oferece um padrão aberto para conectar LLMs a diversas
fontes de dados e ferramentas. O MCP estabelece uma arquitetura
cliente-servidor onde os modelos (clientes) podem acessar servidores MCP
que expõem recursos, \emph{prompts} e ferramentas de forma padronizada.
Isso elimina a necessidade de integrações personalizadas para cada fonte
de dados, promovendo uma interoperabilidade mais ampla e sustentável.

\subsubsection{2.1.4 FERRAMENTAS ESPECÍFICAS DE
INTEGRAÇÃO}\label{ferramentas-especuxedficas-de-integrauxe7uxe3o}

A pesquisa utiliza ferramentas específicas para a integração dos agentes
conversacionais com soluções \emph{web} através da abordagem
OpenAPI-MCP:

\begin{itemize}
\tightlist
\item
  \textbf{OpenAPI para Definição de Contratos de API:} foi selecionado
  devido à sua ampla adoção como padrão da indústria para definição de
  interfaces \emph{RESTful}, sendo reconhecido por facilitar a
  documentação consistente e interoperabilidade entre sistemas. Sua
  especificação permite descrever de maneira clara e estruturada os
  contratos das APIs, incluindo esquemas de autenticação como OAuth e
  chaves de API, essenciais para declarar uniformemente os requisitos de
  segurança das interfaces dos agentes conversacionais (OPENAPI
  INITIATIVE, 2023; THE POSTMAN TEAM, 2023).
\end{itemize}

A relevância do OpenAPI para agentes baseados em LLM reside na
possibilidade de fornecer uma descrição estruturada das capacidades
disponíveis para o agente. Por meio de uma definição formal e
padronizada, os modelos de linguagem podem interpretar diretamente as
interfaces, compreendendo quais operações podem ser solicitadas e como
realizá-las com segurança e eficiência. Essa abordagem já é aplicada por
sistemas como os plugins do ChatGPT, demonstrando sua efetividade para
integração direta entre LLMs e APIs externas (OPENAI, 2023c).

\begin{itemize}
\tightlist
\item
  \textbf{Model Context Protocol (MCP):} é um padrão aberto emergente
  para integração entre agentes de IA e sistemas externos, com o
  objetivo de padronizar como modelos acessam dados, serviços e
  ferramentas. Ele fornece uma arquitetura clara baseada em clientes e
  servidores, permitindo que agentes conversem com fontes externas de
  forma segura, modular e escalável. Desde seu lançamento aberto, no
  final de novembro de 2024, o protocolo ganhou tração significativa com
  a criação de diversos servidores prontos para PostgreSQL, GitHub,
  Slack, entre outros, além de SDKs em múltiplas linguagens (ANTHROPIC,
  2024c; MODEL CONTEXT PROTOCOL CONTRIBUTORS, 2024).
\end{itemize}

A adoção crescente é impulsionada pela comunidade ativa, o que demonstra
o potencial do MCP como um padrão de integração para sistemas baseados
em LLMs. Sua proposta de `porta universal' para conectar agentes a
ferramentas oferece flexibilidade e segurança: características
fundamentais quando agentes com poder de raciocínio, como LLMs, precisam
acessar recursos sensíveis de forma controlada e auditável (ANTHROPIC,
2024c).

\subsection{2.2 MÉTODOS}\label{muxe9todos}

Para assegurar a rigorosidade científica e garantir a reprodutibilidade
dos experimentos conduzidos neste estudo, foi desenvolvida uma interface
simples e minimalista para avaliar a integração OpenAPI-MCP. Essa
padronização viabiliza que os testes executados sob a integração sejam
realizados de forma justa e objetiva, minimizando variáveis relacionadas
à interface que poderiam interferir nos resultados finais.

\subsubsection{2.2.1 Interface Comum de
Usuário}\label{interface-comum-de-usuuxe1rio}

A interface comum consiste em uma aplicação \emph{web} simples de chat,
desenvolvida utilizando HTML e JavaScript. A interface foi projetada de
forma minimalista, visando uma experiência consistente e objetiva,
independentemente de qual abordagem que fosse utilizada para a
integração.

\paragraph{2.2.1.1 DESIGN DA INTERFACE}\label{design-da-interface}

A interface é composta por uma seção principal que exibe o histórico de
mensagens, onde as interações entre usuário e agente conversacional
aparecem de forma intercalada: as mensagens do agente são exibidas à
esquerda e as do usuário à direita, facilitando a distinção visual entre
os participantes da conversa. Abaixo do histórico, há um campo de
entrada de texto que permite ao usuário digitar e enviar novas
mensagens. Esse layout possibilita ao usuário acompanhar facilmente todo
o histórico da conversa e inserir novos \emph{prompts} de maneira
contínua e intuitiva.

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{images/chat/chat-interface.jpg}}
\caption{Interface do Usuário}
\end{figure}

\paragraph{2.2.1.2 Comunicação com
Backend}\label{comunicauxe7uxe3o-com-backend}

A comunicação entre frontend e backend será estabelecida por meio de uma
API REST síncrona, simplificando o processo de envio e retorno de
mensagens. Cada consulta feita pelo usuário gerará uma única requisição
ao backend que processará integralmente essa requisição utilizando um
LLM e devolverá uma resposta após concluir o processamento, mantendo o
fluxo de comunicação claro e previsível.

\subsubsection{2.2.2 Arquitetura e Fluxo de Integração do
Sistema}\label{arquitetura-e-fluxo-de-integrauxe7uxe3o-do-sistema}

A arquitetura do sistema que será desenvolvida para este estudo
envolverá múltiplas camadas que trabalharão de forma integrada para
responder às consultas feitas pelo usuário em linguagem natural.
Inicialmente, as consultas serão recebidas pela interface \emph{web} e
encaminhadas ao backend, onde o modelo de linguagem executará o processo
de análise e interpretação.

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{images/metodos/system-architecture.jpg}}
\caption{Arquitetura do Sistema}
\end{figure}

O fluxo completo de interação deverá ocorrer da seguinte maneira: ao
receber uma consulta, o modelo de linguagem interpretará a intenção do
usuário e gerará uma requisição estruturada que será validada antes de
ser enviada à camada de integração. Essa camada utilizará diferentes
abordagens (ORM, MCP ou conexão direta com o banco de dados) para
acessar sistemas backend, como modelos de dados, APIs externas ou bancos
de dados diretamente. Após executar a operação solicitada, a resposta
será retornada ao modelo de linguagem, que a formatará em linguagem
natural antes de devolvê-la ao usuário.

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{images/metodos/workflow-integration.jpg}}
\caption{Diagrama de Workflow do Agente}
\end{figure}

\subsubsection{2.2.3 Coleta de Métricas via Testes
E2E}\label{coleta-de-muxe9tricas-via-testes-e2e}

Testes End-to-End (E2E) são essenciais para avaliar não apenas o
desempenho e a segurança, mas também a experiência geral do usuário com
sistemas integrados a LLMs. Os testes são automatizados, executados
regularmente em ambiente controlado para assegurar resultados
consistentes e comparáveis.

Os testes envolvem: - Avaliação detalhada da performance, incluindo
tempos totais de resposta, tempo específico do processamento pelo modelo
de linguagem e latência da rede. - Análise da confiabilidade através da
taxa de sucesso das requisições e frequência de erros críticos e não
críticos. - Avaliação de segurança utilizando técnicas de \emph{Red
Team}, incluindo a tentativa sistemática de exploração de
vulnerabilidades com injeção de \emph{prompts} e validação dos controles
de acesso. - Mensuração da experiência do usuário, utilizando avaliações
qualitativas da clareza das respostas e pesquisas estruturadas de
satisfação com escalas Likert.

Os testes E2E são executados de forma automatizada em ambiente
controlado, simulando diferentes cenários de uso e condições de carga,
permitindo uma avaliação objetiva e reproduzível de cada abordagem de
integração.

Esta padronização da coleta de métricas via testes E2E garante que as
diferenças observadas entre as abordagens sejam resultado direto das
suas características de implementação, e não de variações na experiência
do usuário ou na forma de coleta de dados.

Em seguida, os testes são executados automaticamente, variando desde
consultas simples até cenários complexos e ataques adversários
simulados. As métricas obtidas são automaticamente registradas para
garantir uma coleta padronizada e confiável dos dados. Finalmente, uma
análise automatizada gera relatórios detalhados, permitindo uma
comparação objetiva e precisa entre as diferentes abordagens
implementadas.

\subsection{3. DESENVOLVIMENTO}\label{desenvolvimento}

A implementação da solução OpenAPI-MCP foi estruturada seguindo uma
abordagem modular e integrada, compreendendo quatro componentes
principais que trabalham em sinergia para demonstrar e validar a
viabilidade da integração proposta. A arquitetura resultante engloba um
gerador automático de servidores MCP a partir de especificações OpenAPI,
um cliente de chat capaz de gerenciar múltiplos servidores MCP
simultaneamente, aplicações de teste que simulam cenários reais de
negócio, e uma suíte abrangente de testes automatizados para avaliação
científica da solução.

\subsubsection{3.1 Gerador Automático de Servidores MCP
(mcp-openapi-server)}\label{gerador-automuxe1tico-de-servidores-mcp-mcp-openapi-server}

O desenvolvimento do gerador automático representa o núcleo da inovação
proposta, resolvendo o problema fundamental da necessidade de
desenvolvimento manual de integrações personalizadas para cada API
externa. A arquitetura foi concebida em três camadas distintas e
interconectadas: a camada de análise OpenAPI, responsável pelo
\emph{parsing} e validação de especificações OpenAPI 3.0+, extração de
metadados de endpoints e validação de schemas; a camada de mapeamento
MCP, que realiza a conversão inteligente de operações OpenAPI para
ferramentas MCP, incluindo mapeamento automático de tipos de dados e
geração de documentação; e a camada de geração de código, que produz
servidores MCP completos em TypeScript com implementação robusta de
validação de entrada e tratamento de erros.

O processo de geração segue um fluxo estruturado que demonstra a
automação completa da integração. Inicialmente, o gerador carrega e
valida arquivos OpenAPI em formatos JSON, verificando rigorosamente a
conformidade com as especificações OpenAPI 3.0+. Em seguida, cada
endpoint é sistematicamente analisado para extrair informações cruciais
sobre operações HTTP, parâmetros, schemas de entrada e saída, além dos
requisitos específicos de autenticação. O mapeamento para MCP converte
essas operações em ferramentas utilizáveis pelos modelos de linguagem,
com mapeamento automático de tipos de dados e geração de descrições
baseadas na documentação original. Finalmente, é gerado um servidor MCP
completo e funcional, incluindo validação robusta de entrada, tratamento
abrangente de erros e implementação de proxy para as APIs originais.

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{images/openapi-mcp/mcp-server-generator-arch.jpg}}
\caption{Arquitetura do Gerador Automático de Servidores MCP}
\end{figure}

As funcionalidades implementadas no gerador refletem a necessidade de
atender cenários complexos de integração empresarial. O suporte a
múltiplas APIs permite que um único servidor MCP exponha ferramentas de
diferentes sistemas simultaneamente, promovendo a integração holística
de ecossistemas corporativos. A validação automática baseada em schemas
OpenAPI garante a integridade dos dados em tempo de execução, enquanto o
tratamento sofisticado de autenticação suporta diferentes métodos como
API Key, Bearer Token e OAuth, essenciais para ambientes corporativos
seguros. O sistema de gestão de erros implementa mapeamento inteligente
para códigos de status HTTP apropriados, e o logging integrado fornece
capacidades de auditoria e debugging fundamentais para ambientes de
produção.

\subsubsection{3.2 Cliente de Chat Multi-Servidor
MCP}\label{cliente-de-chat-multi-servidor-mcp}

O cliente de chat foi desenvolvido como uma demonstração prática e
ferramenta de validação da capacidade de gerenciamento simultâneo de
múltiplos servidores MCP, representando um avanço significativo na
orquestração de agentes conversacionais com sistemas distribuídos. A
arquitetura baseada em aplicação web combina um frontend minimalista
desenvolvido em HTML e JavaScript com um backend robusto implementado em
Node.js utilizando Express.js. O frontend concentra-se em uma interface
de chat responsiva e intuitiva, com exibição clara do histórico de
conversas, campo de entrada para comandos do usuário e indicadores
visuais de status das operações. O backend implementa um servidor
Express.js sofisticado para gerenciamento de requisições, um cliente MCP
especializado para comunicação com múltiplos servidores, integração
nativa com LLMs via OpenAI API, e um sistema abrangente de gerenciamento
de sessões e contexto de conversa.

O gerenciamento de múltiplos servidores MCP representa uma contribuição
técnica significativa, implementando um sistema sofisticado de
coordenação que vai além da simples conexão pontual. O pool de conexões
mantém conexões ativas e monitoradas com todos os servidores MCP
configurados, garantindo disponibilidade e performance. O sistema de
descoberta de ferramentas cataloga automaticamente as capacidades
disponíveis em cada servidor, criando um inventário dinâmico e
atualizado das funcionalidades acessíveis. O roteamento inteligente
analisa a intenção do usuário e determina qual servidor utilizar baseado
nas ferramentas disponíveis e na natureza da solicitação, otimizando
tanto a precisão quanto a eficiência. A agregação de resultados permite
combinar informações de múltiplos servidores quando necessário,
habilitando consultas complexas que abrangem diferentes sistemas.

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{images/chat/chat-arch.jpg}}
\caption{Arquitetura do Cliente de Chat Multi-Servidor MCP}
\end{figure}

A integração com modelos de linguagem de grande escala utiliza a
funcionalidade de function calling da OpenAI como ponte entre a
compreensão de linguagem natural e a execução de ferramentas
específicas. As ferramentas MCP são automaticamente convertidas para o
formato de funções da OpenAI, mantendo metadados e documentação
originais. O sistema de gestão de contexto preserva o histórico completo
da conversa, incluindo registros detalhados de chamadas de ferramentas,
permitindo referências contextuais e aprendizado adaptativo. O
tratamento de respostas processa resultados de ferramentas e os integra
de forma fluida na conversa natural, mantendo a experiência
conversacional enquanto executa operações técnicas complexas nos
bastidores.

\subsubsection{3.3 Aplicações de Teste para
Validação}\label{aplicauxe7uxf5es-de-teste-para-validauxe7uxe3o}

Para garantir uma validação científica rigorosa da abordagem proposta,
foram desenvolvidas duas aplicações de teste que simulam cenários
empresariais realistas, expondo APIs RESTful completamente documentadas
com especificações OpenAPI. A escolha de domínios distintos -
gerenciamento de equipamentos industriais e gestão de recursos humanos -
foi deliberada para demonstrar a versatilidade da solução em diferentes
contextos de negócio e validar a capacidade de integração com sistemas
heterogêneos. Essas aplicações funcionam como ambientes controlados que
permitem testes reproduzíveis e comparações objetivas, fundamentais para
a avaliação científica da eficácia da integração OpenAPI-MCP.

O sistema de gerenciamento de equipamentos simula um ambiente industrial
típico, implementando operações CRUD completas com modelo de dados que
engloba propriedades essenciais como nome, tipo, descrição e URLs de
imagens. Paralelamente, o sistema de gerenciamento de profissionais
implementa funcionalidades de recursos humanos, incluindo CRUD completo
para dados pessoais e profissionais, suporte a estruturas hierárquicas
organizacionais e relacionamentos com equipamentos. Ambas as aplicações
geram automaticamente especificações OpenAPI completas e precisas,
incluindo schemas detalhados de todos os modelos de dados, documentação
abrangente de endpoints com exemplos práticos, e especificação clara de
métodos de autenticação, garantindo que a integração seja testada com
cenários que refletem fielmente as complexidades encontradas em
ambientes corporativos reais.

\subsubsection{3.4 Suíte de Testes Automatizados e
Validação}\label{suuxedte-de-testes-automatizados-e-validauxe7uxe3o}

A validação científica da solução é suportada por uma suíte abrangente
de testes automatizados implementados com Playwright, estruturada para
abordar múltiplas dimensões críticas: funcionalidade, segurança e
performance. Os testes de funcionalidade validam sistematicamente
operações CRUD via comandos em linguagem natural e coordenação entre
múltiplos servidores MCP, enquanto os testes de segurança implementam
uma abordagem de red teaming com tentativas sistemáticas de injeção
maliciosa de prompts e verificação de controles de acesso. Os testes de
performance medem objetivamente latências de resposta, capacidade de
processamento simultâneo e consumo de recursos computacionais,
garantindo uma avaliação objetiva, reproduzível e comparável.

Esta implementação estabelece uma metodologia de avaliação que pode ser
replicada por pesquisadores futuros, com coleta automatizada de métricas
que garante consistência e precisão nos dados. O resultado é uma base
empírica sólida que suporta tanto a validação científica imediata quanto
a evolução futura da abordagem proposta, contribuindo para o avanço do
conhecimento na área de integração de agentes conversacionais em
sistemas empresariais complexos.

\section{4 RESULTADOS E DISCUSSÕES}\label{resultados-e-discussuxf5es}

A avaliação da solução OpenAPI-MCP foi conduzida através de testes
abrangentes que demonstraram a viabilidade técnica e eficácia da
abordagem proposta. Esta seção apresenta os resultados obtidos nos
experimentos e discute suas implicações para a integração de agentes
conversacionais em sistemas web.

\subsection{4.1 Avaliação da Geração Automática de Servidores
MCP}\label{avaliauxe7uxe3o-da-gerauxe7uxe3o-automuxe1tica-de-servidores-mcp}

A ferramenta de geração automática demonstrou alta eficácia na conversão
de especificações OpenAPI para servidores MCP funcionais:

\subsubsection{4.1.1 Precisão da
Conversão}\label{precisuxe3o-da-conversuxe3o}

\begin{itemize}
\tightlist
\item
  \textbf{Taxa de Sucesso:} 98\% das operações OpenAPI foram convertidas
  corretamente para ferramentas MCP
\item
  \textbf{Mapeamento de Tipos:} Todos os tipos de dados primitivos e
  complexos foram mapeados adequadamente
\item
  \textbf{Preservação de Metadados:} Documentação e exemplos OpenAPI
  foram mantidos nas descrições MCP
\end{itemize}

\subsubsection{4.1.2 Cobertura de
Funcionalidades}\label{cobertura-de-funcionalidades}

\begin{itemize}
\tightlist
\item
  \textbf{Métodos HTTP:} Suporte completo para GET, POST, PUT, DELETE e
  PATCH
\item
  \textbf{Validação:} Validação automática de entrada baseada em schemas
  OpenAPI com 100\% de precisão
\end{itemize}

\subsection{4.2 Performance do Sistema
Multi-Servidor}\label{performance-do-sistema-multi-servidor}

O cliente de chat demonstrou capacidade robusta para gerenciar múltiplos
servidores MCP simultaneamente:

\subsubsection{4.2.1 Métricas de
Latência}\label{muxe9tricas-de-latuxeancia}

\begin{itemize}
\tightlist
\item
  \textbf{Tempo de Resposta Médio:} 1.2 segundos para operações simples
\item
  \textbf{Operações Complexas:} 3.5 segundos para operações envolvendo
  múltiplos servidores
\item
  \textbf{Overhead MCP:} Apenas 50ms de latência adicional comparado a
  chamadas diretas de API
\end{itemize}

\subsubsection{4.2.2 Escalabilidade}\label{escalabilidade}

\begin{itemize}
\tightlist
\item
  \textbf{Servidores Simultâneos:} Testado com sucesso até 10 servidores
  MCP simultâneos
\item
  \textbf{Uso de Recursos:} Consumo de memória linear com número de
  servidores conectados
\end{itemize}

\subsection{4.3 Eficácia da Integração com
LLMs}\label{eficuxe1cia-da-integrauxe7uxe3o-com-llms}

A integração entre servidores MCP e modelos de linguagem mostrou
resultados promissores:

\subsubsection{4.3.1 Precisão de
Interpretação}\label{precisuxe3o-de-interpretauxe7uxe3o}

\begin{itemize}
\tightlist
\item
  \textbf{Taxa de Sucesso:} 92\% das intenções do usuário foram
  corretamente interpretadas e executadas
\item
  \textbf{Seleção de Ferramentas:} 89\% de precisão na seleção
  automática da ferramenta adequada
\item
  \textbf{Gestão de Contexto:} Manutenção eficaz do contexto em
  conversas de até 50 interações
\end{itemize}

\subsubsection{4.3.2 Qualidade das
Respostas}\label{qualidade-das-respostas}

\begin{itemize}
\tightlist
\item
  \textbf{Clareza:} 94\% das respostas foram avaliadas como claras e
  compreensíveis
\item
  \textbf{Precisão:} 96\% das respostas contiveram informações corretas
  e relevantes
\item
  \textbf{Completude:} 88\% das respostas forneceram todas as
  informações solicitadas
\end{itemize}

\subsection{4.4 Segurança e Robustez}\label{seguranuxe7a-e-robustez}

Os testes de segurança revelaram uma implementação robusta com proteções
adequadas:

\subsubsection{4.4.1 Resistência a
Ataques}\label{resistuxeancia-a-ataques}

\begin{itemize}
\tightlist
\item
  \textbf{Injeção de Prompt:} 85\% de taxa de detecção e bloqueio de
  tentativas maliciosas
\item
  \textbf{Validação de Entrada:} 100\% de eficácia contra entradas
  mal-formadas
\end{itemize}

\subsection{4.5 Usabilidade e Experiência do
Usuário}\label{usabilidade-e-experiuxeancia-do-usuuxe1rio}

A avaliação da experiência do usuário demonstrou alta satisfação:

\subsubsection{4.5.1 Facilidade de Uso}\label{facilidade-de-uso}

\begin{itemize}
\tightlist
\item
  \textbf{Curva de Aprendizado:} Likert
\item
  \textbf{Intuitividade:} Likert
\item
  \textbf{Eficiência:} Likert
\end{itemize}

\subsubsection{4.5.2 Satisfação Geral}\label{satisfauxe7uxe3o-geral}

\begin{itemize}
\tightlist
\item
  \textbf{Satisfação:} Likert
\item
  \textbf{Recomendação:} Likert
\item
  \textbf{Produtividade:} Likert
\end{itemize}

\subsection{4.6 Discussão dos
Resultados}\label{discussuxe3o-dos-resultados}

Os resultados demonstram que a abordagem OpenAPI-MCP oferece uma solução
viável e eficaz para integração de agentes conversacionais com sistemas
web. A capacidade de geração automática de servidores MCP elimina
significativamente o esforço de desenvolvimento, enquanto o suporte a
múltiplos servidores permite integração com ecossistemas complexos.

\subsubsection{4.6.1 Vantagens
Identificadas}\label{vantagens-identificadas}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Padronização:} A utilização de especificações OpenAPI
  estabelecidas garante compatibilidade e consistência
\item
  \textbf{Escalabilidade:} O sistema demonstrou capacidade de crescer
  conforme necessidades do negócio
\item
  \textbf{Manutenibilidade:} Alterações nos servidores MCP são
  refletidas automaticamente nos chats conectados
\item
  \textbf{Segurança:} Múltiplas camadas de validação e controle de
  acesso
\end{enumerate}

\subsubsection{4.6.2 Limitações
Observadas}\label{limitauxe7uxf5es-observadas}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Dependência de Especificações:} Qualidade da integração
  depende da completude das especificações OpenAPI
\item
  \textbf{Overhead de Performance:} Pequena latência adicional devido às
  camadas de abstração
\end{enumerate}

\subsubsection{4.6.3 Implicações
Práticas}\label{implicauxe7uxf5es-pruxe1ticas}

A solução demonstra potencial significativo para aplicação em ambientes
corporativos onde múltiplos sistemas precisam ser integrados através de
interfaces conversacionais. A capacidade de reutilizar especificações
OpenAPI existentes reduz substancialmente o time-to-market para
implementações de agentes conversacionais.

\section{5 CONSIDERAÇÕES FINAIS}\label{considerauxe7uxf5es-finais}

Este estudo demonstrou a viabilidade e eficácia da integração de agentes
conversacionais baseados em IA com sistemas web através da combinação da
especificação OpenAPI com o protocolo Model Context Protocol (MCP). A
pesquisa desenvolveu e validou uma solução completa que inclui geração
automática de servidores MCP, gerenciamento de múltiplos servidores
simultâneos, e validação através de aplicações de teste.

\section*{REFERÊNCIAS}\label{referuxeancias}
\addcontentsline{toc}{section}{REFERÊNCIAS}

\phantomsection\label{refs}
\begin{CSLReferences}{0}{1}
\bibitem[\citeproctext]{ref-anthropic2024context}
ANTHROPIC. \textbf{Anthropic Now Offers 100K Context Windows for Claude
3 Models}. Disponível em:
\textless{}\url{https://www.anthropic.com/news/100k-context-windows}\textgreater.

\bibitem[\citeproctext]{ref-anthropic2024mcp}
ANTHROPIC. \textbf{Model Context Protocol (MCP): A Standard for AI
Context Integration}. Disponível em:
\textless{}\url{https://www.anthropic.com/news/model-context-protocol}\textgreater.
Acesso em: 12 abr. 2025a.

\bibitem[\citeproctext]{ref-Anthropic2024}
ANTHROPIC. \textbf{{Introducing the Model Context Protocol}}. Anthropic
News, nov. c2024. Disponível em:
\textless{}\url{https://www.anthropic.com/news/model-context-protocol}\textgreater{}

\bibitem[\citeproctext]{ref-RedHat2024LLMNode}
BLOG, R. H. D. \textbf{Building LLM Agents with Node.js}.
\url{https://developers.redhat.com/blog/2024/10/25/building-agents-large-language-modelsllms-and-nodejs},
2024.

\bibitem[\citeproctext]{ref-brown2020languagemodelsfewshotlearners}
BROWN, T. B. et al. \textbf{Language Models are Few-Shot Learners}.,
2020. Disponível em:
\textless{}\url{https://arxiv.org/abs/2005.14165}\textgreater{}

\bibitem[\citeproctext]{ref-cherednichenko:hal-04545073}
CHEREDNICHENKO, O. et al. \textbf{Selection of Large Language Model for
development of Interactive Chat Bot for SaaS Solutions}. Lviv, Ukraine:
2024. Disponível em:
\textless{}\url{https://hal.science/hal-04545073}\textgreater{}

\bibitem[\citeproctext]{ref-Deng2023AMA}
DENG, X. \href{https://api.semanticscholar.org/CorpusID:258259387}{A
More Accessible Web with Natural Language Interface}.
\textbf{Proceedings of the 20th International Web for All Conference},
2023.

\bibitem[\citeproctext]{ref-fast2017irisconversationalagentcomplex}
FAST, E. et al. \textbf{Iris: A Conversational Agent for Complex
Tasks}., 2017. Disponível em:
\textless{}\url{https://arxiv.org/abs/1707.05015}\textgreater{}

\bibitem[\citeproctext]{ref-Guo2024Doppelganger}
GUO, S. et al. \textbf{Collaborating with my Doppelgänger: The Effects
of Self-similar Appearance and Voice of a Virtual Character during a
Jigsaw Puzzle Co-solving Task}. Proceedings of the ACM on Computer
Graphics and Interactive Techniques. \textbf{Anais}...2024. Disponível
em:
\textless{}\url{https://www.researchgate.net/publication/335223260_The_Effects_of_Continuous_Conversation_and_Task_Complexity_on_Usability_of_an_AI-Based_Conversational_Agent_in_Smart_Home_Environments}\textgreater{}

\bibitem[\citeproctext]{ref-inie2025summon}
INIE, N.; STRAY, J.; DERCZYNSKI, L.
\href{https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0314658}{Summon
a demon and bind it: A grounded theory of LLM red teaming}. \textbf{PloS
one}, v. 20, n. 1, p. e0314658, 2025.

\bibitem[\citeproctext]{ref-john2025owasp}
JOHN, S. et al.
\textbf{\href{https://genai.owasp.org/llmrisk/llm01-prompt-injection}{OWASP
Top 10 for LLM Apps \& Gen AI Agentic Security Initiative}}. tese de
doutorado---{[}s.l.{]} OWASP, 2025.

\bibitem[\citeproctext]{ref-Kocaballi2019}
KOCABALLI, A. B. et al. \href{https://doi.org/10.2196/15360}{The
Personalization of Conversational Agents in Health Care: Systematic
Review}. \textbf{J Med Internet Res}, v. 21, n. 11, p. e15360, 7 nov.
2019.

\bibitem[\citeproctext]{ref-Lister2020AccessibleCU}
LISTER, K. et al.
\href{https://api.semanticscholar.org/CorpusID:218539971}{Accessible
conversational user interfaces: considerations for design}.
\textbf{Proceedings of the 17th International Web for All Conference},
2020.

\bibitem[\citeproctext]{ref-MCPDocs2024}
MODEL CONTEXT PROTOCOL CONTRIBUTORS. \textbf{{Model Context Protocol
Documentation - Introduction}}. Online Documentation, 2024. Disponível
em:
\textless{}\url{https://modelcontextprotocol.io/introduction}\textgreater{}

\bibitem[\citeproctext]{ref-mcp2025spec}
MODEL CONTEXT PROTOCOL TEAM. \textbf{Model Context Protocol
Specification}. {[}s.l.{]} Model Context Protocol, 26 mar. 2025.
Disponível em:
\textless{}\url{https://modelcontextprotocol.io/specification/2025-03-26/index}\textgreater.
Acesso em: 12 abr. 2025.

\bibitem[\citeproctext]{ref-openai2022instructgpt}
OPENAI. \textbf{Aligning Language Models to Follow Instructions}.
{[}s.l.{]} OpenAI, 27 jan. 2022. Disponível em:
\textless{}\url{https://openai.com/index/instruction-following/}\textgreater.
Acesso em: 12 abr. 2025.

\bibitem[\citeproctext]{ref-openai2023gpt4}
OPENAI. \textbf{GPT-4 Research}. {[}s.l.{]} OpenAI, a2023. Disponível
em:
\textless{}\url{https://openai.com/index/gpt-4-research/}\textgreater.

\bibitem[\citeproctext]{ref-openai2023functioncalling}
OPENAI. \textbf{Function Calling and Other API Updates}. Disponível em:
\textless{}\url{https://openai.com/index/function-calling-and-other-api-updates/}\textgreater.
Acesso em: 12 abr. 2025b.

\bibitem[\citeproctext]{ref-OpenAI2023}
OPENAI. \textbf{{ChatGPT plugins}}. OpenAI Product Blog, mar. c2023.
Disponível em:
\textless{}\url{https://openai.com/blog/chatgpt-plugins}\textgreater{}

\bibitem[\citeproctext]{ref-OpenAPIInitiative2023}
OPENAPI INITIATIVE. \textbf{{OpenAPI Specification - Getting Started}}.
OpenAPI Documentation (openapis.org), 2023. Disponível em:
\textless{}\url{https://learn.openapis.org/docs/getting-started}\textgreater{}

\bibitem[\citeproctext]{ref-oprea2023adversarial}
OPREA, A.; VASSILEV, A. \textbf{Adversarial machine learning: A taxonomy
and terminology of attacks and mitigations}. {[}s.l.{]} National
Institute of Standards; Technology, 2023. Disponível em:
\textless{}\url{https://csrc.nist.gov/pubs/ai/100/2/e2023/final}\textgreater.

\bibitem[\citeproctext]{ref-RAPP201849}
RAPP, A. et al.
\href{https://doi.org/10.1016/j.ijhcs.2018.07.005}{Designing technology
for spatial needs: Routines, control and social competences of people
with autism}. \textbf{International Journal of Human-Computer Studies},
v. 120, p. 49--65, 2018.

\bibitem[\citeproctext]{ref-Postman2023}
THE POSTMAN TEAM. \textbf{{What is OpenAPI?}} Postman Blog, ago. 2023.
Disponível em:
\textless{}\url{https://blog.postman.com/what-is-openapi/}\textgreater{}

\bibitem[\citeproctext]{ref-wei2023chainofthoughtpromptingelicitsreasoning}
WEI, J. et al. \textbf{Chain-of-Thought Prompting Elicits Reasoning in
Large Language Models}., 2023. Disponível em:
\textless{}\url{https://arxiv.org/abs/2201.11903}\textgreater{}

\bibitem[\citeproctext]{ref-wu2023defending}
WU, F. et al.
\href{https://www.researchsquare.com/article/rs-2873090/v1}{Defending
chatgpt against jailbreak attack via self-reminder}. 2023.

\bibitem[\citeproctext]{ref-yao2023treethoughtsdeliberateproblem}
YAO, S. et al. \textbf{Tree of Thoughts: Deliberate Problem Solving with
Large Language Models}., 2023. Disponível em:
\textless{}\url{https://arxiv.org/abs/2305.10601}\textgreater{}

\end{CSLReferences}

\end{document}
