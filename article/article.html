<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang xml:lang>
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>AGENTE CONVERSACIONAL PARA INTERAÇÃO APRIMORADA EM SISTEMAS</title>
  <style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}

ul.task-list[class]{list-style: none;}
ul.task-list li input[type="checkbox"] {
font-size: inherit;
width: 0.8em;
margin: 0 0.8em 0.2em -1.6em;
vertical-align: middle;
}
.display.math{display: block; text-align: center; margin: 0.5rem auto;}
</style>
  <style type="text/css">
html {
font-size: 100%;
overflow-y: scroll;
-webkit-text-size-adjust: 100%;
-ms-text-size-adjust: 100%;
}
body {
color: #444;
font-family: Georgia, Palatino, "Palatino Linotype", Times, "Times New Roman",
serif;
font-size: 12px;
line-height: 1.7;
padding: 1em;
margin: auto;
max-width: 42em;
background: #fefefe;
}
a {
color: #0645ad;
text-decoration: none;
}
a:visited {
color: #0b0080;
}
a:hover {
color: #06e;
}
a:active {
color: #faa700;
}
a:focus {
outline: thin dotted;
}
*::-moz-selection {
background: rgba(255, 255, 0, 0.3);
color: #000;
}
*::selection {
background: rgba(255, 255, 0, 0.3);
color: #000;
}
a::-moz-selection {
background: rgba(255, 255, 0, 0.3);
color: #0645ad;
}
a::selection {
background: rgba(255, 255, 0, 0.3);
color: #0645ad;
}
p {
margin: 1em 0;
}
img {
max-width: 100%;
}
h1,
h2,
h3,
h4,
h5,
h6 {
color: #111;
line-height: 125%;
margin-top: 2em;
font-weight: normal;
}
h4,
h5,
h6 {
font-weight: bold;
}
h1 {
font-size: 2.5em;
}
h2 {
font-size: 2em;
}
h3 {
font-size: 1.5em;
}
h4 {
font-size: 1.2em;
}
h5 {
font-size: 1em;
}
h6 {
font-size: 0.9em;
}
blockquote {
color: #666666;
margin: 0;
padding-left: 3em;
border-left: 0.5em #eee solid;
}
hr {
display: block;
height: 2px;
border: 0;
border-top: 1px solid #aaa;
border-bottom: 1px solid #eee;
margin: 1em 0;
padding: 0;
}
pre,
code,
kbd,
samp {
color: #000;
font-family: monospace, monospace;
_font-family: "courier new", monospace;
font-size: 0.98em;
}
pre {
white-space: pre;
white-space: pre-wrap;
word-wrap: break-word;
}
b,
strong {
font-weight: bold;
}
dfn {
font-style: italic;
}
ins {
background: #ff9;
color: #000;
text-decoration: none;
}
mark {
background: #ff0;
color: #000;
font-style: italic;
font-weight: bold;
}
sub,
sup {
font-size: 75%;
line-height: 0;
position: relative;
vertical-align: baseline;
}
sup {
top: -0.5em;
}
sub {
bottom: -0.25em;
}
ul,
ol {
margin: 1em 0;
padding: 0 0 0 2em;
}
li p:last-child {
margin-bottom: 0;
}
ul ul,
ol ol {
margin: 0.3em 0;
}
dl {
margin-bottom: 1em;
}
dt {
font-weight: bold;
margin-bottom: 0.8em;
}
dd {
margin: 0 0 0.8em 2em;
}
dd:last-child {
margin-bottom: 0;
}
img {
border: 0;
-ms-interpolation-mode: bicubic;
vertical-align: middle;
}
figure {
display: block;
text-align: center;
margin: 1em 0;
}
figure img {
border: none;
margin: 0 auto;
}
figcaption {
font-size: 0.8em;
font-style: italic;
margin: 0 0 0.8em;
}
table {
margin-bottom: 2em;
border-bottom: 1px solid #ddd;
border-right: 1px solid #ddd;
border-spacing: 0;
border-collapse: collapse;
}
table th {
padding: 0.2em 1em;
background-color: #eee;
border-top: 1px solid #ddd;
border-left: 1px solid #ddd;
}
table td {
padding: 0.2em 1em;
border-top: 1px solid #ddd;
border-left: 1px solid #ddd;
vertical-align: top;
}
.author {
font-size: 1.2em;
text-align: center;
}
@media only screen and (min-width: 480px) {
body {
font-size: 14px;
}
}
@media only screen and (min-width: 768px) {
body {
font-size: 16px;
}
}
@media print {
* {
background: transparent !important;
color: black !important;
filter: none !important;
-ms-filter: none !important;
}
body {
font-size: 12pt;
max-width: 100%;
}
a,
a:visited {
text-decoration: underline;
}
hr {
height: 1px;
border: 0;
border-bottom: 1px solid black;
}
a[href]:after {
content: " (" attr(href) ")";
}
abbr[title]:after {
content: " (" attr(title) ")";
}
.ir a:after,
a[href^="javascript:"]:after,
a[href^="#"]:after {
content: "";
}
pre,
blockquote {
border: 1px solid #999;
padding-right: 1em;
page-break-inside: avoid;
}
tr,
img {
page-break-inside: avoid;
}
img {
max-width: 100% !important;
}
@page :left {
margin: 15mm 20mm 15mm 10mm;
}
@page :right {
margin: 15mm 10mm 15mm 20mm;
}
p,
h2,
h3 {
orphans: 3;
widows: 3;
}
h2,
h3 {
page-break-after: avoid;
}
}
</style>
</head>
<body>
<header id="title-block-header">
<h1 class="title"><strong>AGENTE CONVERSACIONAL PARA INTERAÇÃO
APRIMORADA EM SISTEMAS</strong></h1>
</header>
<h3 id="artigo-em-produção---checklist-de-produção">Artigo em produção -
Checklist de produção</h3>
<ul class="task-list">
<li><label><input type="checkbox"></input>Edição do artigo</label>
<ul class="task-list">
<li><label><input type="checkbox"></input>Aplicar ABNT</label></li>
<li><label><input type="checkbox"></input>Aplicar formatação da
SATC</label></li>
</ul></li>
<li><label><input type="checkbox"></input>Escrita</label>
<ul class="task-list">
<li><label><input type="checkbox"></input>Resumo</label>
<ul class="task-list">
<li><label><input type="checkbox" checked></input>Esqueleto</label></li>
<li><label><input type="checkbox"></input>Revisão após finalizar o
artigo</label></li>
</ul></li>
<li><label><input type="checkbox" checked></input>Introdução (preciso de
umas referências)</label></li>
<li><label><input type="checkbox"></input>Material e métodos</label>
<ul class="task-list">
<li><label><input type="checkbox" checked></input>Abordagem
geral</label></li>
<li><label><input type="checkbox"></input>Procedimento experimental de cada
alternativa</label></li>
</ul></li>
<li><label><input type="checkbox"></input>Resultados e discussão</label></li>
<li><label><input type="checkbox"></input>Considerações finais</label></li>
<li><label><input type="checkbox"></input>Referências</label>
<ul class="task-list">
<li><label><input type="checkbox"></input>Formatar ABNT</label></li>
</ul></li>
</ul></li>
</ul>
<p><strong>Lucas de Castro Zanoni</strong><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p><strong>Thyerri Fernandes Mezzari</strong><a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>Resumo: Este trabalho apresenta o desenvolvimento de um agente
conversacional baseado em inteligência artificial para aprimorar a
interação entre usuários e sistemas. Utilizando técnicas avançadas de
processamento de linguagem natural, o agente proposto visa simplificar a
comunicação em interfaces complexas, proporcionando uma experiência
digital unificada e adaptável às necessidades dos usuários. A
metodologia inclui o desenvolvimento, implementação e avaliação do
agente em ambientes reais de uso. Os resultados demonstram que a solução
proposta contribui significativamente para a melhoria da acessibilidade
e usabilidade dos sistemas, reduzindo barreiras de interação e
promovendo uma comunicação mais fluida e intuitiva.</p>
<p><strong>Palavras-chaves:</strong> agente conversacional, interação,
sistema, inteligência artificial.</p>
<h1 id="introdução">1 INTRODUÇÃO</h1>
<p>A evolução das interfaces de usuário tem gerado uma diversidade de
padrões de design e usabilidade, resultando frequentemente em barreiras
para a plena acessibilidade e interação dos usuários com os sistemas
digitais. Com o aumento da complexidade do frontend e a multiplicidade
de paradigmas de interação, muitos usuários enfrentam dificuldades
significativas para utilizar efetivamente as funcionalidades oferecidas
pelos sistemas computacionais modernos <span class="citation" data-cites="RAPP201849">[@RAPP201849]</span> <span class="citation" data-cites="Kocaballi2019">[@Kocaballi2019]</span>.</p>
<p>Nesse cenário, os agentes conversacionais baseados em inteligência
artificial emergem como uma alternativa promissora para simplificar a
comunicação entre humanos e máquinas, oferecendo uma camada
intermediária de interação que pode traduzir comandos em linguagem
natural para ações específicas no sistema.</p>
<p>Estudos recentes têm demonstrado que agentes conversacionais podem
aprimorar significativamente a experiência do usuário ao simplificar
interações com sistemas complexos <span class="citation" data-cites="fast2017irisconversationalagentcomplex">[@fast2017irisconversationalagentcomplex]</span>.
Além disso, a implementação de interfaces baseadas em linguagem natural
tem mostrado potencial para melhorar a usabilidade em contextos
domésticos e inteligentes, reduzindo o tempo e o esforço necessários
para completar tarefas complexas <span class="citation" data-cites="Guo2024Doppelganger">[@Guo2024Doppelganger]</span>. Ademais,
tais interfaces oferecem vantagens consideráveis em termos de
acessibilidade, permitindo uma comunicação mais inclusiva e adaptável a
usuários com diferentes necessidades especiais <span class="citation" data-cites="Lister2020AccessibleCU">[@Lister2020AccessibleCU]</span>
<span class="citation" data-cites="Deng2023AMA">[@Deng2023AMA]</span>.</p>
<p>A problemática central desta pesquisa reside na questão: de que forma
um agente conversacional baseado em IA pode potencializar a interação
entre usuários e sistemas, promovendo uma comunicação fluida mesmo em
ambientes com interfaces complexas? Essa pergunta reflete a necessidade
crescente de soluções que democratizem o acesso à tecnologia, reduzindo
a curva de aprendizado necessária para a utilização de sistemas
especializados e tornando-os mais acessíveis para diferentes perfis de
usuários.</p>
<p>Adicionalmente, trabalhos recentes indicam que avanços na arquitetura
de modelos de IA, como o uso de transformers sem camadas de
normalização, podem influenciar positivamente o desempenho e a
eficiência desses agentes <span class="citation" data-cites="Zhu2025DyT">[@Zhu2025DyT]</span>.</p>
<p>A relevância deste estudo evidencia-se pelo potencial transformador
que os agentes conversacionais representam para a área de interação
humano-computador. Ao implementar um sistema intermediário capaz de
interpretar linguagem natural e traduzi-la em ações específicas dentro
de um sistema, cria-se uma ponte que permite aos usuários interagir de
forma mais intuitiva e natural com as tecnologias digitais. Esta
abordagem tem o potencial de mitigar as barreiras impostas por
interfaces complexas, contribuindo para uma maior inclusão digital e
para a melhoria da experiência do usuário em diversos contextos de
aplicação.</p>
<!-- ## Abordagens de Integração para Análise

### 1. Conexão Direta com Banco de Dados
- [ ] Análise de Vantagens:
  - Acesso direto aos dados brutos
  - Menor latência na recuperação de dados
  - Sem necessidade de camadas API intermediárias
  - Controle completo sobre padrões de acesso a dados
- [ ] Análise de Desvantagens:
  - Preocupações com segurança no acesso direto ao BD
  - Necessidade de lidar com múltiplos tipos de BD
  - Geração complexa de SQL
  - Requer compreensão profunda do esquema
  - Alta manutenção quando o esquema do BD muda

### 2. Integração via Plugin ORM
- [ ] Análise de Vantagens:
  - Aproveita a lógica da aplicação existente
  - Melhor segurança através das camadas do ORM
  - Manutenção mais fácil (segue atualizações da aplicação)
  - Uso mais eficiente de recursos
- [ ] Análise de Desvantagens:
  - Específico para linguagem/framework
  - Requer modificação do código existente
  - Limitado às capacidades do ORM
  - Maior complexidade de implementação para desenvolvedores

### 3. Integração via API/Swagger
- [ ] Análise de Vantagens:
  - Utiliza infraestrutura de API existente
  - Melhor segurança (camadas de autenticação existentes)
  - Agnóstico quanto a linguagem/framework
  - Mais fácil de implementar em sistemas existentes
- [ ] Análise de Desvantagens:
  - Maior latência (requisições HTTP)
  - Sobrecarga de rede
  - Depende da disponibilidade da API
  - Pode requerer múltiplas requisições para operações complexas

### 4. Model Context Protocol (MCP)
- [ ] Análise de Vantagens:
  - Forma padronizada de definir interações com ferramentas
  - Flexível e extensível
  - Agnóstico quanto a linguagem
  - Clara separação de responsabilidades
- [ ] Análise de Desvantagens:
  - Necessita geração dinâmica de servidor
  - Infraestrutura adicional necessária
  - Tecnologia mais recente com menos suporte da comunidade
  - Implementação complexa para ferramentas dinâmicas

## Estrutura de Pesquisa

### 1. Fundamentação Teórica
- [ ] Revisão de padrões existentes de integração com LLMs
- [ ] Análise de arquiteturas de integração de sistemas
- [ ] Considerações de segurança em integrações com IA
- [ ] Métricas e considerações de desempenho

### 2. Análise de Implementação
- [ ] Para cada abordagem:
  - [ ] Design arquitetural
  - [ ] Considerações de segurança
  - [ ] Implicações de desempenho
  - [ ] Complexidade de implementação
  - [ ] Requisitos de manutenção
  - [ ] Aspectos de escalabilidade

### 3. Prova de Conceito
- [ ] Implementação em pequena escala de cada abordagem
- [ ] Cenário de teste padronizado
- [ ] Coleta de métricas de desempenho
- [ ] Análise de segurança
- [ ] Avaliação da experiência do usuário

### 4. Critérios de Avaliação
- [ ] Métricas de desempenho
- [ ] Avaliação de segurança
- [ ] Complexidade de implementação
- [ ] Sobrecarga de manutenção
- [ ] Potencial de escalabilidade
- [ ] Experiência do usuário
- [ ] Esforço de integração

### 5. Framework de Comparação
- [ ] Metodologia de comparação padronizada
- [ ] Métricas quantitativas
- [ ] Análise qualitativa
- [ ] Considerações específicas de casos de uso -->
<h1 id="procedimento-experimental">2 PROCEDIMENTO EXPERIMENTAL</h1>
<p>Este trabalho adota uma abordagem metodológica estruturada em
múltiplas etapas para investigar e avaliar diferentes métodos de
integração entre agentes conversacionais baseados em LLMs (Large
Language Models) e sistemas computacionais. A pesquisa se desenvolve
através de uma análise comparativa de quatro abordagens distintas de
integração, cada uma com suas características, vantagens e limitações
específicas.</p>
<p>O processo investigativo inicia-se com uma revisão sistemática da
literatura sobre integrações entre LLMs e sistemas, estabelecendo uma
base teórica sólida para a análise subsequente. Em seguida, são
exploradas quatro abordagens principais de integração: (1) conexão
direta com banco de dados, permitindo consultas e manipulações diretas;
(2) integração via plugins ORM, facilitando o acesso através de camadas
de abstração existentes; (3) integração via API/Swagger, utilizando
interfaces padronizadas de comunicação; e (4) integração via Model
Context Protocol (MCP), explorando um paradigma emergente de comunicação
entre LLMs e sistemas.</p>
<p>Para cada abordagem, será desenvolvida uma prova de conceito que
demonstre sua viabilidade técnica e permita uma avaliação objetiva de
seus aspectos funcionais e não-funcionais. A avaliação seguirá critérios
predefinidos, incluindo desempenho, segurança, facilidade de
implementação, manutenibilidade e experiência do usuário. Os resultados
serão documentados e analisados de forma sistemática, permitindo uma
comparação objetiva entre as diferentes abordagens.</p>
<h2 id="materiais">2.1 MATERIAIS</h2>
<p>Para garantir a rigorosidade científica e a reprodutibilidade dos
experimentos conduzidos neste estudo, é essencial uma seleção criteriosa
dos materiais e ferramentas utilizados. Esta seção detalha os recursos
específicos empregados na condução desta pesquisa, justificando sua
escolha baseada na eficiência, popularidade, robustez e aplicabilidade
prática dentro do contexto dos agentes conversacionais e integração de
sistemas.</p>
<h3 id="node.js-para-desenvolvimento-das-provas-de-conceito">2.1.1
NODE.JS PARA DESENVOLVIMENTO DAS PROVAS DE CONCEITO</h3>
<p>Node.js foi escolhido como plataforma principal para o
desenvolvimento das provas de conceito devido à sua comprovada eficácia
na integração de sistemas baseados em inteligência artificial (IA),
especialmente com agentes conversacionais e Large Language Models
(LLMs). A plataforma é amplamente adotada devido à sua arquitetura
orientada a eventos e capacidade de gerenciar eficientemente múltiplas
conexões simultâneas, essencial para aplicações que exigem respostas
rápidas em tempo real <span class="citation" data-cites="cherednichenko:hal-04545073">[@cherednichenko:hal-04545073]</span>.</p>
<p>O Hugging Face fornece bibliotecas JavaScript específicas compatíveis
com Node.js, como o <code>@huggingface/inference</code>, permitindo
acesso direto a mais de 100 mil modelos pré-treinados com suporte a
TypeScript. Isso simplifica significativamente a integração com IA,
destacando a robustez técnica e facilidade de adoção do Node.js em
aplicações modernas <span class="citation" data-cites="HuggingFace2024">[@HuggingFace2024]</span>.</p>
<p>Grandes empresas também reforçam a relevância de Node.js ao
disponibilizarem SDKs específicos, como o da IBM para o Watsonx, lançado
em 2023. Este SDK facilita o uso direto de modelos generativos robustos
da IBM em aplicações Node.js, destacando sua relevância estratégica no
ambiente empresarial <span class="citation" data-cites="IBM2023WatsonxSDK">[@IBM2023WatsonxSDK]</span>.</p>
<p>Adicionalmente, a documentação oficial do Node.js ressalta sua
capacidade superior de lidar com streaming de dados através de streams e
pipelines. Essa funcionalidade permite transmitir resultados
incrementais de IA aos clientes com baixa latência, tornando-o ideal
para chatbots e serviços em tempo real que dependem de respostas
imediatas <span class="citation" data-cites="Nodejs2024Docs">[@Nodejs2024Docs]</span>.</p>
<p>Por fim, relatórios da Red Hat destacam que o uso eficiente da
arquitetura assíncrona do Node.js possibilita a criação de agentes
baseados em LLMs com alta performance e escalabilidade. Isso garante um
gerenciamento eficiente de múltiplas operações paralelas, essencial para
aplicações intensivas em IA e integração com APIs externas <span class="citation" data-cites="RedHat2024LLMNode">[@RedHat2024LLMNode]</span>.</p>
<h3 id="testes-end-to-end-e2e">2.1.2 TESTES END-TO-END (E2E)</h3>
<p>O Framework de Gerenciamento de Riscos de IA do NIST <span class="citation" data-cites="oprea2023adversarial">[@oprea2023adversarial]</span> destaca
a importância de avaliar o desempenho de sistemas de IA de forma
abrangente, defendendo que testes de integração devem avaliar os
sistemas de ponta a ponta para identificar erros de integração e
garantir a precisão das respostas em cenários realistas. Testes
rigorosos como esses não apenas identificam problemas de integração, mas
também asseguram às partes interessadas que o sistema se comporta
conforme o esperado em condições do mundo real.</p>
<p>A injeção de prompt representa um risco significativo em implantações
de LLMs em nosso cenário, no qual o modelo possui acesso a dados e
sistemas potencialmente críticos, incluindo, ocasionalmente, conexões
diretas com dados brutos de banco de dados. O guia de riscos da OWASP
<span class="citation" data-cites="john2025owasp">[@john2025owasp]</span> classifica a injeção
de prompt como uma ameaça crítica à segurança, destacando a necessidade
de procedimentos de teste rigorosos para garantir que agentes
conversacionais baseados em LLMs não revelem inadvertidamente dados
sensíveis ou contornem restrições do sistema quando expostos a entradas
maliciosas. Recentemente, Wu et al. (2023) <span class="citation" data-cites="wu2023defending">[@wu2023defending]</span> demonstraram que
ataques de jailbreak — um tipo avançado de injeção de prompt — podem
burlar as salvaguardas éticas de modelos como o ChatGPT em até 67% dos
casos, gerando conteúdos prejudiciais como extorsão e desinformação.</p>
<p>Com isso em mente, o uso de testes E2E pode ser utilizado para
avaliar a resiliência da implementação ao simular entradas adversárias,
processo conhecido como red teaming. Segundo Inie et al. (2025) <span class="citation" data-cites="inie2025summon">[@inie2025summon]</span>, o
red teaming desafia sistematicamente sistemas de IA com prompts
adversários projetados para testar seus limites e mecanismos de
segurança. Ao encapsular consultas do usuário com lembretes de
responsabilidade ética (e.g., “Você deve ser um ChatGPT responsável”), o
método reduziu a taxa de sucesso de jailbreaks para 19%, mantendo a
funcionalidade padrão do modelo — um resultado validado através de
testes E2E em 540 cenários adversarialmente projetados <span class="citation" data-cites="wu2023defending">[@wu2023defending]</span>.</p>
<p>Testes de robustez, como os propostos pelo framework CheckList <span class="citation" data-cites="ribeiro2020beyond">[@ribeiro2020beyond]</span>, complementam
ainda mais os testes E2E ao variar sistematicamente as entradas — como
paráfrases, negações ou ruído — para avaliar a consistência e a precisão
do modelo em diferentes cenários. Esse método garante que sistemas
baseados em LLM lidem de forma confiável com interações diversas dos
usuários, atributo essencial para manter a confiança dos usuários e a
estabilidade operacional, especialmente em aplicações críticas de
negócios ou voltadas à segurança.</p>
<h3 id="modelos-de-linguagem-de-grande-escala-llms">2.1.3 MODELOS DE
LINGUAGEM DE GRANDE ESCALA (LLMs)</h3>
<p>Os modelos de linguagem (LLMs), incluindo tecnologias como OpenAI
GPT, Anthropic e modelos disponibilizados pela Google, são essenciais
neste estudo devido à sua capacidade de interpretar e gerar linguagem
natural de forma avançada e eficaz. Estes modelos foram selecionados por
sua performance comprovada e ampla adoção em pesquisas acadêmicas e no
mercado corporativo, proporcionando um sólido embasamento para as
funcionalidades de interação do agente conversacional.</p>
<h4 id="histórico-do-desenvolvimento-de-llms-20182023">2.1.3.1 HISTÓRICO
DO DESENVOLVIMENTO DE LLMS (2018–2023)</h4>
<p>Nos últimos cinco anos, os Modelos de Linguagem de Grande Escala
(LLMs) evoluíram rapidamente, a partir da arquitetura Transformer. O
lançamento do BERT (2018) mostrou avanços em compreensão textual,
enquanto a série GPT demonstrou fortes capacidades generativas. O GPT-3
(2020), com 175 bilhões de parâmetros, evidenciou habilidades emergentes
de aprendizado com poucos exemplos (few-shot), ampliando o escopo de
tarefas possíveis por meio de simples instruções em linguagem natural
<span class="citation" data-cites="brown2020languagemodelsfewshotlearners">[@brown2020languagemodelsfewshotlearners]</span>.</p>
<p>A partir de 2022, o foco da pesquisa passou a ser o aprimoramento do
raciocínio e alinhamento dos LLMs. Técnicas como Chain-of-Thought
prompting permitiram que os modelos resolvessem problemas complexos de
forma mais eficaz <span class="citation" data-cites="wei2023chainofthoughtpromptingelicitsreasoning">[@wei2023chainofthoughtpromptingelicitsreasoning]</span>.
O uso de Reinforcement Learning from Human Feedback (RLHF), como nos
modelos InstructGPT e posteriormente ChatGPT, melhorou a capacidade dos
LLMs de seguir instruções com mais segurança e consistência. Esses
avanços estabeleceram as bases para o uso dos LLMs como interfaces
conversacionais robustas em cenários de integração com sistemas <span class="citation" data-cites="openai2022instructgpt">[@openai2022instructgpt]</span>.</p>
<h4 id="extensão-de-janela-de-contexto">2.1.3.2 EXTENSÃO DE JANELA DE
CONTEXTO</h4>
<p>Com o avanço dos modelos, observou-se uma tendência significativa no
aumento das janelas de contexto — a quantidade de tokens que um LLM pode
processar em uma única interação. Modelos como o Claude 3 já alcançam
até 100.000 tokens <span class="citation" data-cites="anthropic2024context">[@anthropic2024context]</span>,
enquanto versões estendidas do GPT-4 suportam até 32.000 tokens <span class="citation" data-cites="openai2023gpt4">[@openai2023gpt4]</span>.
Esse aumento permite que os modelos processem documentos extensos,
múltiplas conversas ou grandes volumes de dados em uma única
solicitação, superando, em muitos casos, abordagens tradicionais
baseadas em retrieval-augmented generation (RAG), especialmente em
tarefas que exigem síntese contextual profunda.</p>
<p>A capacidade de manter longos contextos é altamente benéfica para
integração com sistemas – um LLM pode manter diálogos prolongados,
lembrar estados extensos ou ingerir bancos de dados e logs inteiros de
uma só vez. No entanto, isso traz custos computacionais consideráveis, e
há esforços contínuos para utilizar essas janelas maiores de forma
eficiente (por exemplo, condensando ou focando a atenção nas partes mais
relevantes) <span class="citation" data-cites="anthropic2024context openai2023gpt4">[@anthropic2024context;
@openai2023gpt4]</span>.</p>
<h4 id="raciocínio-aprimorado-e-compreensão-profunda-deep-thinking">2.1.3.3
RACIOCÍNIO APRIMORADO E COMPREENSÃO PROFUNDA (DEEP THINKING)</h4>
<p>Os LLMs mais recentes apresentam avanços significativos em
raciocínio, planejamento e resolução de tarefas complexas. Técnicas como
o Chain-of-Thought prompting, que induz os modelos a pensar em etapas
intermediárias, mostraram ganhos substanciais em tarefas que exigem
múltiplos passos lógicos <span class="citation" data-cites="wei2023chainofthoughtpromptingelicitsreasoning">[@wei2023chainofthoughtpromptingelicitsreasoning]</span>.
Além disso, abordagens como tree-of-thought e self-reflection permitem
que os modelos reavaliem suas respostas e melhorem sua própria
performance iterativamente. Esses avanços tornam os LLMs mais confiáveis
para tarefas que exigem raciocínio profundo e tomada de decisão
estruturada, fundamentais para integração com sistemas complexos <span class="citation" data-cites="yao2023treethoughtsdeliberateproblem">[@yao2023treethoughtsdeliberateproblem]</span>.</p>
<h4 id="uso-de-ferramentas-em-tempo-real-e-interação-com-sistemas">2.1.3.4
USO DE FERRAMENTAS EM TEMPO REAL E INTERAÇÃO COM SISTEMAS</h4>
<p>O avanço dos LLMs em ambientes de produção foi impulsionado por
recursos como o function calling da OpenAI <span class="citation" data-cites="openai2023functioncalling">[@openai2023functioncalling]</span>.
Essa funcionalidade permite que os modelos interpretem solicitações em
linguagem natural e as convertam em chamadas de funções estruturadas,
conforme definido por esquemas JSON fornecidos pelo desenvolvedor. Por
exemplo, ao receber uma instrução como “agende uma reunião para amanhã
às 14h”, o modelo pode gerar uma chamada de função com os parâmetros
apropriados para interagir com uma API de calendário, sem depender de
engenharia de prompt ou extração de texto.</p>
<p>Essa abordagem, semelhante ao modelo escrever código para utilizar
ferramentas, melhora significativamente a confiabilidade em cenários de
integração, permitindo que o modelo obtenha dados estruturados de bancos
de dados, chame APIs de negócios, envie e-mails, entre outras ações, em
vez de apenas tentar adivinhar a resposta <span class="citation" data-cites="openai2023functioncalling">[@openai2023functioncalling]</span>.</p>
<p>Complementando essa capacidade, o Model Context Protocol (MCP),
desenvolvido pela Anthropic <span class="citation" data-cites="mcp2025spec anthropic2024mcp">[@mcp2025spec;
@anthropic2024mcp]</span>, oferece um padrão aberto para conectar LLMs a
diversas fontes de dados e ferramentas. O MCP estabelece uma arquitetura
cliente-servidor onde os modelos (clientes) podem acessar servidores MCP
que expõem recursos, prompts e ferramentas de forma padronizada. Isso
elimina a necessidade de integrações personalizadas para cada fonte de
dados, promovendo uma interoperabilidade mais ampla e sustentável.</p>
<h3 id="ferramentas-específicas-de-integração">2.1.4 FERRAMENTAS
ESPECÍFICAS DE INTEGRAÇÃO</h3>
<p>A pesquisa investigou quatro abordagens distintas para a integração
dos agentes conversacionais com sistemas computacionais, utilizando
ferramentas específicas para cada uma:</p>
<ul>
<li><p><strong>PostgreSQL para Conexão Direta com Banco de
Dados:</strong> foi escolhido para a conexão direta com banco de dados
devido à sua ampla adoção e aceitação pela comunidade de
desenvolvedores, evidenciada pela pesquisa do Stack Overflow Developer
Survey, onde apareceu como o banco de dados mais admirado e desejado por
desenvolvedores em 2023 <span class="citation" data-cites="enterprisedb2023postgresql">[@enterprisedb2023postgresql]</span>.
Além disso, décadas de desenvolvimento ativo e testes rigorosos pela
comunidade garantem ao PostgreSQL uma reputação sólida em termos de
integridade dos dados e tolerância a falhas. Assim, utilizar PostgreSQL
assegura que os dados do agente conversacional sejam gerenciados por uma
infraestrutura confiável, escalável e amplamente reconhecida pela
indústria, com vasto suporte operacional disponível <span class="citation" data-cites="enterprisedb2023postgresql enterprisedb2023security">[@enterprisedb2023postgresql;
@enterprisedb2023security]</span>.</p></li>
<li><p><strong>Sequelize para Integração via ORM:</strong> Este ORM foi
selecionado como ferramenta ORM devido ao seu amplo uso em aplicações
Node.js, sendo uma das bibliotecas mais populares para gerenciamento de
banco de dados nessa plataforma, com cerca de 27 mil estrelas no GitHub
e mais de meio milhão de repositórios que o utilizam <span class="citation" data-cites="sequelize2024">[@sequelize2024]</span>.
Empresas reconhecidas, como PayPal e Red Hat, utilizam Sequelize em
produção, reforçando sua credibilidade e robustez. Além disso, o uso de
Sequelize proporciona segurança adicional ao prevenir automaticamente
ataques de SQL injection por meio de queries parametrizadas, oferecendo
também suporte para caches e consultas em SQL bruto quando necessário,
equilibrando segurança com flexibilidade e desempenho <span class="citation" data-cites="eversql2023orms">[@eversql2023orms]</span>.</p></li>
<li><p><strong>OpenAPI para Integração via API/Swagger:</strong> foi
selecionado devido à sua ampla adoção como padrão da indústria para
definição de interfaces RESTful, sendo reconhecido por facilitar a
documentação consistente e interoperabilidade entre sistemas. Sua
especificação permite descrever de maneira clara e estruturada os
contratos das APIs, incluindo esquemas de autenticação como OAuth e
chaves de API, essenciais para declarar uniformemente os requisitos de
segurança das interfaces dos agentes conversacionais <span class="citation" data-cites="OpenAPIInitiative2023 Postman2023">[@OpenAPIInitiative2023;
@Postman2023]</span>.</p></li>
</ul>
<p>A relevância do OpenAPI para agentes baseados em LLM reside na
possibilidade de fornecer uma descrição estruturada das capacidades
disponíveis para o agente. Por meio de uma definição formal e
padronizada, os modelos de linguagem podem interpretar diretamente as
interfaces, compreendendo quais operações podem ser solicitadas e como
realizá-las com segurança e eficiência. Essa abordagem já é aplicada por
sistemas como os plugins do ChatGPT, demonstrando sua efetividade para
integração direta entre LLMs e APIs externas <span class="citation" data-cites="OpenAI2023">[@OpenAI2023]</span>.</p>
<ul>
<li><strong>Model Context Protocol (MCP):</strong> é um padrão aberto
emergente para integração entre agentes de IA e sistemas externos, com o
objetivo de padronizar como modelos acessam dados, serviços e
ferramentas. Ele fornece uma arquitetura clara baseada em clientes e
servidores MCP, permitindo que agentes conversem com fontes externas de
forma segura, modular e escalável. Desde seu lançamento aberto, entre
fevereiro e abril de 2025, o protocolo ganhou tração significativa com a
criação de diversos servidores prontos para PostgreSQL, GitHub, Slack,
entre outros, além de SDKs em múltiplas linguagens <span class="citation" data-cites="Anthropic2024 MCPDocs2024">[@Anthropic2024;
@MCPDocs2024]</span>.</li>
</ul>
<p>A adoção crescente é impulsionada pela comunidade ativa, o que
demonstra o potencial do MCP como um padrão de integração para sistemas
baseados em LLMs. Sua proposta de “porta universal” para conectar
agentes a ferramentas oferece flexibilidade e segurança —
características fundamentais quando agentes com poder de raciocínio,
como LLMs, precisam acessar recursos sensíveis de forma controlada e
auditável <span class="citation" data-cites="Anthropic2024">[@Anthropic2024]</span>.</p>
<h2 id="métodos">2.2 MÉTODOS</h2>
<p>Em métodos deve ter uma explicação minuciosa, detalhada, rigorosa e
exata de toda ação desenvolvida no método (caminho) do trabalho de
pesquisa. É necessário descrever quais equipamentos serão utilizados e
todo o procedimento experimental.</p>
<p>É a explicação do tipo de pesquisa, do instrumental utilizado
(softwares, equipamentos, questionários, entrevistas, etc.), do tempo
previsto, do laboratório, das formas de tabulação e tratamento dos
dados, enfim, de tudo aquilo que se utilizou ou será utilizado no
trabalho.</p>
<p><strong>A seguir regras de formatação para o desenvolvimento do
artigo:</strong></p>
<p>É de extrema importância realizar uma pesquisa bibliográfica, do tema
a ser estudado, baseada em periódicos nacionais e internacionais
(artigos, anais de congressos, revistas especializadas) e também em
livros, teses e dissertações para direcionar os procedimentos
experimentais adotados e os resultados e discussões obtidos. Essas
referências deveram ser citadas ao longo do artigo.</p>
<p>É importante compreender que cópias de trechos deverão ser feitas de
acordo com as normas da ABNT, ou seja: citações diretas e/ou indiretas,
curtas e/ou longas. Cópia de trechos e/ou na íntegra sem os devidos
créditos é considerado plágio (lei nº 9.610, de 19.02.98, que altera,
atualiza e consolida a legislação sobre direitos autorais). Não se
esqueça de nomear a seção.</p>
<h1 id="resultados-e-discussões">3 RESULTADOS E DISCUSSÕES</h1>
<p>Nos Resultados e Discussões, deve-se apresentar os resultados obtidos
no Procedimento Experimental e fazer uma discussão e análise sobre os
mesmos sempre que possível referenciando a literatura pesquisada.</p>
<h1 id="considerações-finais">4 CONSIDERAÇÕES FINAIS</h1>
<p>Etapa esta que servirá para você evidenciar as conquistas alcançadas
com o estudo e indicar as limitações e as reconsiderações. Além disso,
você poderá apontar a relação entre fatos verificados e teoria e mostrar
a contribuição da pesquisa para o meio acadêmico, empresarial e/ou para
o desenvolvimento da ciência e tecnologia. Além disso, você poderá
sugerir temas complementares a sua pesquisa para estudos futuros.
Responda aqui a sua pergunta-problema de pesquisa.</p>
<h1 id="referências">REFERÊNCIAS</h1>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>Graduando em Engenharia de software no semestre letivo
de 2024-2. E-mail: castro.lucas290@gmail.com<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Professor do Centro Universitário UniSATC E-mail:
thyerri.mezzari@satc.edu.br<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
